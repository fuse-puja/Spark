{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Different Types of Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initialize PySpark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"day3\").getOrCreate()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Chipotle dataset into a Spark DataFrame\n",
    "data_path = \"../data/titanic.csv\"  # Replace with the actual path\n",
    "titanic_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Load the Chipotle dataset into a Spark DataFrame\n",
    "data_path = '../data/chipotle.csv' # Replace with the actual path\n",
    "chipotle_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Load the Chipotle dataset into a Spark DataFrame\n",
    "data_path = '../data/kalimati_tarkari_dataset.csv' # Replace with the actual path\n",
    "kalimati_df = spark.read.csv(data_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- choice_description: string (nullable = true)\n",
      " |-- item_price: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- SN: integer (nullable = true)\n",
      " |-- Commodity: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Unit: string (nullable = true)\n",
      " |-- Minimum: double (nullable = true)\n",
      " |-- Maximum: double (nullable = true)\n",
      " |-- Average: double (nullable = true)\n",
      "\n",
      "None None None\n"
     ]
    }
   ],
   "source": [
    "# Display schema of all the dataset\n",
    "print(titanic_df.printSchema(),chipotle_df.printSchema(),kalimati_df.printSchema())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Spark Types:\n",
    "\n",
    "Question: Load the \"titanic\" dataset and convert the \"Fare\" column from double to integer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- fare: integer (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert the \"Fare\" column from double to integer\n",
    "titanic_df = titanic_df.withColumn(\"fare\", col(\"fare\").cast(\"int\"))\n",
    "\n",
    "# Display schema\n",
    "titanic_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Booleans:\n",
    "\n",
    "Question: Load the \"titanic\" dataset and add a new column \"IsAdult\" that indicates whether a passenger is an adult (age >= 18) or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+-----+--------+-------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|fare|Cabin|Embarked|IsAdult|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+-----+--------+-------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7| null|       S|   true|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|  71|  C85|       C|   true|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|   7| null|       S|   true|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|  53| C123|       S|   true|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8| null|       S|   true|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877|   8| null|       Q|   null|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|  51|  E46|       S|   true|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909|  21| null|       S|  false|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|  11| null|       S|   true|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|  30| null|       C|  false|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|  16|   G6|       S|  false|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26| C103|       S|   true|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8| null|       S|   true|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082|  31| null|       S|   true|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406|   7| null|       S|  false|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|  16| null|       S|   true|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652|  29| null|       Q|  false|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|  13| null|       S|   null|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|  18| null|       S|   true|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|   7| null|       C|   null|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+-----+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Add a new column \"IsAdult\" that indicates whether a passenger is an adult (age >= 18) or not.\n",
    "titanic_df.withColumn(\"IsAdult\", expr(\"Age >= 18\")).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Numbers:\n",
    "\n",
    "Question: Load the \"titanic\" dataset and calculate the average age of male and female passengers separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|   Sex|        AverageAge|\n",
      "+------+------------------+\n",
      "|female|27.915708812260537|\n",
      "|  male| 30.72664459161148|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculate the average age of male and female passengers separately\n",
    "average_age_by_sex = titanic_df.groupBy(\"Sex\").agg(avg(\"Age\").alias(\"AverageAge\")).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Strings:\n",
    "\n",
    "Question: Load the \"chipotle\" dataset and find the item names containing the word \"Chicken.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+--------------------+--------------------+----------+\n",
      "|_c0|order_id|quantity|           item_name|  choice_description|item_price|\n",
      "+---+--------+--------+--------------------+--------------------+----------+\n",
      "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |\n",
      "|  5|       3|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $10.98 |\n",
      "| 11|       6|       1|Chicken Crispy Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "| 12|       6|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "| 13|       7|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $11.25 |\n",
      "| 16|       8|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
      "| 17|       9|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.49 |\n",
      "| 19|      10|       1|        Chicken Bowl|[Tomatillo Red Ch...|    $8.75 |\n",
      "| 23|      12|       1|     Chicken Burrito|[[Tomatillo-Green...|   $10.98 |\n",
      "| 26|      13|       1|        Chicken Bowl|[Roasted Chili Co...|    $8.49 |\n",
      "| 29|      15|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
      "| 35|      18|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "| 36|      18|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "| 42|      20|       1|        Chicken Bowl|[Roasted Chili Co...|   $11.25 |\n",
      "| 44|      20|       1|  Chicken Salad Bowl|[Fresh Tomato Sal...|    $8.75 |\n",
      "| 45|      21|       1|     Chicken Burrito|[Tomatillo-Red Ch...|   $10.98 |\n",
      "| 52|      24|       1|     Chicken Burrito|[Roasted Chili Co...|   $10.98 |\n",
      "| 63|      28|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.75 |\n",
      "| 68|      30|       1|     Chicken Burrito|[Tomatillo-Red Ch...|   $10.98 |\n",
      "| 73|      33|       1|     Chicken Burrito|[Tomatillo Red Ch...|    $8.75 |\n",
      "+---+--------+--------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:56:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , order_id, quantity, item_name, choice_description, item_price\n",
      " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/user/Documents/Fusemachines/Spark%2029.08.23/data/chipotle.csv\n"
     ]
    }
   ],
   "source": [
    "# Find the item names containing the word \"Chicken.\"\n",
    "chipotle_df.filter(chipotle_df.item_name.contains(\"Chicken\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions:\n",
    "\n",
    "Question: Load the \"chipotle\" dataset and find the items with names that start with \"Ch\" followed by any character.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+--------------------+--------------------+----------+\n",
      "|_c0|order_id|quantity|           item_name|  choice_description|item_price|\n",
      "+---+--------+--------+--------------------+--------------------+----------+\n",
      "|  0|       1|       1|Chips and Fresh T...|                null|    $2.39 |\n",
      "|  3|       1|       1|Chips and Tomatil...|                null|    $2.39 |\n",
      "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |\n",
      "|  5|       3|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $10.98 |\n",
      "| 10|       5|       1| Chips and Guacamole|                null|    $4.45 |\n",
      "| 11|       6|       1|Chicken Crispy Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "| 12|       6|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "| 13|       7|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $11.25 |\n",
      "| 14|       7|       1| Chips and Guacamole|                null|    $4.45 |\n",
      "| 15|       8|       1|Chips and Tomatil...|                null|    $2.39 |\n",
      "| 16|       8|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
      "| 17|       9|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.49 |\n",
      "| 19|      10|       1|        Chicken Bowl|[Tomatillo Red Ch...|    $8.75 |\n",
      "| 20|      10|       1| Chips and Guacamole|                null|    $4.45 |\n",
      "| 23|      12|       1|     Chicken Burrito|[[Tomatillo-Green...|   $10.98 |\n",
      "| 25|      13|       1|Chips and Fresh T...|                null|    $2.39 |\n",
      "| 26|      13|       1|        Chicken Bowl|[Roasted Chili Co...|    $8.49 |\n",
      "| 29|      15|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
      "| 30|      15|       1|Chips and Tomatil...|                null|    $2.39 |\n",
      "| 35|      18|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "+---+--------+--------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:56:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , order_id, quantity, item_name, choice_description, item_price\n",
      " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/user/Documents/Fusemachines/Spark%2029.08.23/data/chipotle.csv\n"
     ]
    }
   ],
   "source": [
    "# Find the items with names that start with \"Ch\" followed by any character\n",
    "chipotle_df.filter(col(\"item_name\").rlike(r'^Ch')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Nulls in Data:\n",
    "\n",
    "Question: Load the \"titanic\" dataset and count the number of passengers with missing age information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passengers with missing age: 177\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Count the number of passengers with missing age information\n",
    "missing_age_titatnic = titanic_df.filter(col(\"Age\").isNull()\n",
    "                                         ).count()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of passengers with missing age:\", missing_age_titatnic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coalesce\n",
    "Question: Utilizing the Chipotle dataset, use the coalesce function to combine the \"item_name\" and \"choice_description\" columns into a new column named \"OrderDetails.\" Display the first 5 rows of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+--------------------+--------------------+----------+--------------------+\n",
      "|_c0|order_id|quantity|           item_name|  choice_description|item_price|        OrderDetails|\n",
      "+---+--------+--------+--------------------+--------------------+----------+--------------------+\n",
      "|  0|       1|       1|Chips and Fresh T...|                null|    $2.39 |Chips and Fresh T...|\n",
      "|  1|       1|       1|                Izze|        [Clementine]|    $3.39 |                Izze|\n",
      "|  2|       1|       1|    Nantucket Nectar|             [Apple]|    $3.39 |    Nantucket Nectar|\n",
      "|  3|       1|       1|Chips and Tomatil...|                null|    $2.39 |Chips and Tomatil...|\n",
      "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |        Chicken Bowl|\n",
      "+---+--------+--------+--------------------+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:56:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , order_id, quantity, item_name, choice_description, item_price\n",
      " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/user/Documents/Fusemachines/Spark%2029.08.23/data/chipotle.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# Coalesce function to combine the \"item_name\" and \"choice_description\" columns into a new column named \"OrderDetails.\"\n",
    "chipotle_df.select(\"*\", coalesce(col(\"item_name\"), \n",
    "                                 col(\"choice_description\")\n",
    "                                 ).alias(\"OrderDetails\")).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ifnull, nullIf, nvl, and nvl2\n",
    "\n",
    "Question: Replace the null values in the \"Age\" column of the Titanic dataset with the average age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|              Age|SibSp|Parch|          Ticket|fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+----+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|             22.0|    1|    0|       A/5 21171|   7| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|             38.0|    1|    0|        PC 17599|  71|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|             26.0|    0|    0|STON/O2. 3101282|   7| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|             35.0|    1|    0|          113803|  53| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|             35.0|    0|    0|          373450|   8| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|29.69911764705882|    0|    0|          330877|   8| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|             54.0|    0|    0|           17463|  51|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|              2.0|    3|    1|          349909|  21| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|             27.0|    0|    2|          347742|  11| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|             14.0|    1|    0|          237736|  30| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|              4.0|    1|    1|         PP 9549|  16|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|             58.0|    0|    0|          113783|  26| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|             20.0|    0|    0|       A/5. 2151|   8| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|             39.0|    1|    5|          347082|  31| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|             14.0|    0|    0|          350406|   7| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|             55.0|    0|    0|          248706|  16| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|              2.0|    4|    1|          382652|  29| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|29.69911764705882|    0|    0|          244373|  13| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|             31.0|    1|    0|          345763|  18| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|29.69911764705882|    0|    0|            2649|   7| null|       C|\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculate the average value of the \"Age\" column\n",
    "average_value = titanic_df.select(avg(\"Age\")).collect()[0][0]\n",
    "\n",
    "# Fill missing values in the \"Age\" column with the calculated average value\n",
    "titanic_nnull = titanic_df.fillna({\"Age\": average_value})\n",
    "\n",
    "# Dislplay the result\n",
    "titanic_nnull.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop\n",
    "\n",
    "Question: Remove the \"Cabin\" column from the Titanic dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|fare|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|  71|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|   7|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|  53|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877|   8|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|  51|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909|  21|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|  11|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|  30|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|  16|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082|  31|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406|   7|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|  16|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652|  29|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|  13|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|  18|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|   7|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DROP COLUMN \"Cabin\"\n",
    "titanic_df = titanic_df.drop(\"Cabin\")\n",
    "\n",
    "#Display the result\n",
    "titanic_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill\n",
    "\n",
    "Question: Fill the null values in the \"Age\" column of the Titanic dataset with a default age of 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|fare|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|  71|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|   7|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|  53|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877|   8|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|  51|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909|  21|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|  11|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|  30|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|  16|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082|  31|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406|   7|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|  16|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652|  29|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|  13|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|  18|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|   7|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default = 30\n",
    "\n",
    "# Fill missing values in the \"Age\" column with the default value of 30\n",
    "titanic_df.fillna(default, subset=[\"Age\"])\n",
    "\n",
    "# Display result\n",
    "titanic_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  replace\n",
    "\n",
    "Question: Replace the gender \"male\" with \"M\" and \"female\" with \"F\" in the \"Sex\" column of the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+---+----+-----+-----+----------------+----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|Sex| Age|SibSp|Parch|          Ticket|fare|Embarked|\n",
      "+-----------+--------+------+--------------------+---+----+-----+-----+----------------+----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  M|22.0|    1|    0|       A/5 21171|   7|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|  F|38.0|    1|    0|        PC 17599|  71|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|  F|26.0|    0|    0|STON/O2. 3101282|   7|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|  F|35.0|    1|    0|          113803|  53|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  M|35.0|    0|    0|          373450|   8|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  M|null|    0|    0|          330877|   8|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  M|54.0|    0|    0|           17463|  51|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  M| 2.0|    3|    1|          349909|  21|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|  F|27.0|    0|    2|          347742|  11|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|  F|14.0|    1|    0|          237736|  30|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|  F| 4.0|    1|    1|         PP 9549|  16|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|  F|58.0|    0|    0|          113783|  26|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  M|20.0|    0|    0|       A/5. 2151|   8|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  M|39.0|    1|    5|          347082|  31|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|  F|14.0|    0|    0|          350406|   7|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|  F|55.0|    0|    0|          248706|  16|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  M| 2.0|    4|    1|          382652|  29|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  M|null|    0|    0|          244373|  13|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|  F|31.0|    1|    0|          345763|  18|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|  F|null|    0|    0|            2649|   7|       C|\n",
      "+-----------+--------+------+--------------------+---+----+-----+-----+----------------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "\n",
    "#Replace the gender \"male\" with \"M\" and \"female\" with \"F\" in the \"Sex\"\n",
    "titanic_df.withColumn(\"Sex\",\\\n",
    "                       when(col(\"Sex\") == \"male\", \"M\")\\\n",
    "                       .otherwise(\"F\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Working with Complex Types: Structs\n",
    "\n",
    "Question: Create a new DataFrame from the Kalimati Tarkari dataset, including a new column \"PriceRange\" that is a struct containing \"Minimum\" and \"Maximum\" prices for each commodity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Complex Types: Arrays\n",
    "Question: Create a new DataFrame from the Kalimati Tarkari dataset, including a new column \"CommodityList\" that is an array of all the commodities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+----+-------+-------+-------+----------------------+\n",
      "|SN |Commodity           |Date      |Unit|Minimum|Maximum|Average|CommodityList         |\n",
      "+---+--------------------+----------+----+-------+-------+-------+----------------------+\n",
      "|0  |Tomato Big(Nepali)  |2013-06-16|Kg  |35.0   |40.0   |37.5   |[Tomato Big(Nepali)]  |\n",
      "|1  |Tomato Small(Local) |2013-06-16|Kg  |26.0   |32.0   |29.0   |[Tomato Small(Local)] |\n",
      "|2  |Potato Red          |2013-06-16|Kg  |20.0   |21.0   |20.5   |[Potato Red]          |\n",
      "|3  |Potato White        |2013-06-16|Kg  |15.0   |16.0   |15.5   |[Potato White]        |\n",
      "|4  |Onion Dry (Indian)  |2013-06-16|Kg  |28.0   |30.0   |29.0   |[Onion Dry (Indian)]  |\n",
      "|5  |Carrot(Local)       |2013-06-16|Kg  |30.0   |35.0   |32.5   |[Carrot(Local)]       |\n",
      "|6  |Cabbage(Local)      |2013-06-16|Kg  |6.0    |10.0   |8.0    |[Cabbage(Local)]      |\n",
      "|7  |Cauli Local         |2013-06-16|Kg  |30.0   |35.0   |32.5   |[Cauli Local]         |\n",
      "|8  |Raddish Red         |2013-06-16|Kg  |35.0   |40.0   |37.5   |[Raddish Red]         |\n",
      "|9  |Raddish White(Local)|2013-06-16|Kg  |25.0   |30.0   |27.5   |[Raddish White(Local)]|\n",
      "|10 |Brinjal Long        |2013-06-16|Kg  |16.0   |18.0   |17.0   |[Brinjal Long]        |\n",
      "|11 |Brinjal Round       |2013-06-16|Kg  |20.0   |22.0   |21.0   |[Brinjal Round]       |\n",
      "|12 |Cow pea(Long)       |2013-06-16|Kg  |20.0   |25.0   |22.5   |[Cow pea(Long)]       |\n",
      "|13 |Green Peas          |2013-06-16|Kg  |55.0   |60.0   |57.5   |[Green Peas]          |\n",
      "|14 |French Bean(Local)  |2013-06-16|Kg  |25.0   |30.0   |27.5   |[French Bean(Local)]  |\n",
      "|15 |Soyabean Green      |2013-06-16|Kg  |60.0   |70.0   |65.0   |[Soyabean Green]      |\n",
      "|16 |Bitter Gourd        |2013-06-16|Kg  |14.0   |16.0   |15.0   |[Bitter Gourd]        |\n",
      "|17 |Bottle Gourd        |2013-06-16|Kg  |15.0   |20.0   |17.5   |[Bottle Gourd]        |\n",
      "|18 |Pointed Gourd(Local)|2013-06-16|Kg  |30.0   |35.0   |32.5   |[Pointed Gourd(Local)]|\n",
      "|19 |Snake Gourd         |2013-06-16|Kg  |25.0   |30.0   |27.5   |[Snake Gourd]         |\n",
      "+---+--------------------+----------+----+-------+-------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Split the \"Commodity\" column by space delimiter \" \" and create a new column \"CommodityList\"\n",
    "k_df = kalimati_df.select(\"*\", split(col(\"Commodity\"), \"   \").alias(\"CommodityList\"))\n",
    "\n",
    "# Display the resultant dataset\n",
    "k_df.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Complex Types: explode\n",
    "\n",
    "Question: Explode the \"CommodityList\" array column from the previous step to generate a new row for each commodity in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+-------+-------+-------+--------------------+\n",
      "|SN |Date      |Unit|Minimum|Maximum|Average|Commodity           |\n",
      "+---+----------+----+-------+-------+-------+--------------------+\n",
      "|0  |2013-06-16|Kg  |35.0   |40.0   |37.5   |Tomato Big(Nepali)  |\n",
      "|1  |2013-06-16|Kg  |26.0   |32.0   |29.0   |Tomato Small(Local) |\n",
      "|2  |2013-06-16|Kg  |20.0   |21.0   |20.5   |Potato Red          |\n",
      "|3  |2013-06-16|Kg  |15.0   |16.0   |15.5   |Potato White        |\n",
      "|4  |2013-06-16|Kg  |28.0   |30.0   |29.0   |Onion Dry (Indian)  |\n",
      "|5  |2013-06-16|Kg  |30.0   |35.0   |32.5   |Carrot(Local)       |\n",
      "|6  |2013-06-16|Kg  |6.0    |10.0   |8.0    |Cabbage(Local)      |\n",
      "|7  |2013-06-16|Kg  |30.0   |35.0   |32.5   |Cauli Local         |\n",
      "|8  |2013-06-16|Kg  |35.0   |40.0   |37.5   |Raddish Red         |\n",
      "|9  |2013-06-16|Kg  |25.0   |30.0   |27.5   |Raddish White(Local)|\n",
      "|10 |2013-06-16|Kg  |16.0   |18.0   |17.0   |Brinjal Long        |\n",
      "|11 |2013-06-16|Kg  |20.0   |22.0   |21.0   |Brinjal Round       |\n",
      "|12 |2013-06-16|Kg  |20.0   |25.0   |22.5   |Cow pea(Long)       |\n",
      "|13 |2013-06-16|Kg  |55.0   |60.0   |57.5   |Green Peas          |\n",
      "|14 |2013-06-16|Kg  |25.0   |30.0   |27.5   |French Bean(Local)  |\n",
      "|15 |2013-06-16|Kg  |60.0   |70.0   |65.0   |Soyabean Green      |\n",
      "|16 |2013-06-16|Kg  |14.0   |16.0   |15.0   |Bitter Gourd        |\n",
      "|17 |2013-06-16|Kg  |15.0   |20.0   |17.5   |Bottle Gourd        |\n",
      "|18 |2013-06-16|Kg  |30.0   |35.0   |32.5   |Pointed Gourd(Local)|\n",
      "|19 |2013-06-16|Kg  |25.0   |30.0   |27.5   |Snake Gourd         |\n",
      "+---+----------+----+-------+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Explode the \"CommodityList\" array column from the previous step to generate a new row for each commodity in the list.\n",
    "exploded_df = k_df.select(\"SN\", \"Date\", \"Unit\", \"Minimum\", \"Maximum\", \"Average\", explode(\"CommodityList\").alias(\"Commodity\"))\n",
    "\n",
    "# Display the resulting dataset\n",
    "exploded_df.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Complex Types: Maps\n",
    "\n",
    "Question: Create a new DataFrame from the Kalimati Tarkari dataset, including a new column \"PriceMap\" that is a map with \"Commodity\" as the key and \"Average\" price as the value.\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+----+-------+-------+-------+------------------------------+\n",
      "|SN |Commodity           |Date      |Unit|Minimum|Maximum|Average|PriceMap                      |\n",
      "+---+--------------------+----------+----+-------+-------+-------+------------------------------+\n",
      "|0  |Tomato Big(Nepali)  |2013-06-16|Kg  |35.0   |40.0   |37.5   |{Tomato Big(Nepali) -> 37.5}  |\n",
      "|1  |Tomato Small(Local) |2013-06-16|Kg  |26.0   |32.0   |29.0   |{Tomato Small(Local) -> 29.0} |\n",
      "|2  |Potato Red          |2013-06-16|Kg  |20.0   |21.0   |20.5   |{Potato Red -> 20.5}          |\n",
      "|3  |Potato White        |2013-06-16|Kg  |15.0   |16.0   |15.5   |{Potato White -> 15.5}        |\n",
      "|4  |Onion Dry (Indian)  |2013-06-16|Kg  |28.0   |30.0   |29.0   |{Onion Dry (Indian) -> 29.0}  |\n",
      "|5  |Carrot(Local)       |2013-06-16|Kg  |30.0   |35.0   |32.5   |{Carrot(Local) -> 32.5}       |\n",
      "|6  |Cabbage(Local)      |2013-06-16|Kg  |6.0    |10.0   |8.0    |{Cabbage(Local) -> 8.0}       |\n",
      "|7  |Cauli Local         |2013-06-16|Kg  |30.0   |35.0   |32.5   |{Cauli Local -> 32.5}         |\n",
      "|8  |Raddish Red         |2013-06-16|Kg  |35.0   |40.0   |37.5   |{Raddish Red -> 37.5}         |\n",
      "|9  |Raddish White(Local)|2013-06-16|Kg  |25.0   |30.0   |27.5   |{Raddish White(Local) -> 27.5}|\n",
      "|10 |Brinjal Long        |2013-06-16|Kg  |16.0   |18.0   |17.0   |{Brinjal Long -> 17.0}        |\n",
      "|11 |Brinjal Round       |2013-06-16|Kg  |20.0   |22.0   |21.0   |{Brinjal Round -> 21.0}       |\n",
      "|12 |Cow pea(Long)       |2013-06-16|Kg  |20.0   |25.0   |22.5   |{Cow pea(Long) -> 22.5}       |\n",
      "|13 |Green Peas          |2013-06-16|Kg  |55.0   |60.0   |57.5   |{Green Peas -> 57.5}          |\n",
      "|14 |French Bean(Local)  |2013-06-16|Kg  |25.0   |30.0   |27.5   |{French Bean(Local) -> 27.5}  |\n",
      "|15 |Soyabean Green      |2013-06-16|Kg  |60.0   |70.0   |65.0   |{Soyabean Green -> 65.0}      |\n",
      "|16 |Bitter Gourd        |2013-06-16|Kg  |14.0   |16.0   |15.0   |{Bitter Gourd -> 15.0}        |\n",
      "|17 |Bottle Gourd        |2013-06-16|Kg  |15.0   |20.0   |17.5   |{Bottle Gourd -> 17.5}        |\n",
      "|18 |Pointed Gourd(Local)|2013-06-16|Kg  |30.0   |35.0   |32.5   |{Pointed Gourd(Local) -> 32.5}|\n",
      "|19 |Snake Gourd         |2013-06-16|Kg  |25.0   |30.0   |27.5   |{Snake Gourd -> 27.5}         |\n",
      "+---+--------------------+----------+----+-------+-------+-------+------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "# Creating new column \"PriceMap\" that is a map with \"Commodity\" as the key and \"Average\" price as the value\n",
    "kalimati_tarkari= kalimati_df.select(\"*\", create_map(col(\"Commodity\"), col(\"Average\")).alias(\"PriceMap\"))\\\n",
    "\n",
    "## Display \"kalimati_tarkari\" DataFrame        \n",
    "kalimati_tarkari.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with JSON\n",
    "\n",
    "Question: Convert the \"kalimati_df\" DataFrame to JSON format and write it to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to Kalimati.json\n"
     ]
    }
   ],
   "source": [
    "# Convert the PySpark DataFrame \"kalimati_df\" to a JSON format and collect the results\n",
    "kalimati_json = kalimati_df.toJSON().collect()\n",
    "\n",
    "filename = \"Kalimati.json\"\n",
    "\n",
    "# Open the file in write mode and write each JSON row to the file\n",
    "with open(filename, \"w\") as f:\n",
    "    for json_row in kalimati_json:\n",
    "        f.write(json_row + \"\\n\")\n",
    "\n",
    "# Print a message indicating that the data has been successfully written to the file\n",
    "print(\"Data written to\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:56:30 WARN TaskSetManager: Stage 117 contains a task of very large size (5701 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+-------+-------+---+----+\n",
      "|Average|           Commodity|      Date|Maximum|Minimum| SN|Unit|\n",
      "+-------+--------------------+----------+-------+-------+---+----+\n",
      "|   37.5|  Tomato Big(Nepali)|2013-06-16|   40.0|   35.0|  0|  Kg|\n",
      "|   29.0| Tomato Small(Local)|2013-06-16|   32.0|   26.0|  1|  Kg|\n",
      "|   20.5|          Potato Red|2013-06-16|   21.0|   20.0|  2|  Kg|\n",
      "|   15.5|        Potato White|2013-06-16|   16.0|   15.0|  3|  Kg|\n",
      "|   29.0|  Onion Dry (Indian)|2013-06-16|   30.0|   28.0|  4|  Kg|\n",
      "|   32.5|       Carrot(Local)|2013-06-16|   35.0|   30.0|  5|  Kg|\n",
      "|    8.0|      Cabbage(Local)|2013-06-16|   10.0|    6.0|  6|  Kg|\n",
      "|   32.5|         Cauli Local|2013-06-16|   35.0|   30.0|  7|  Kg|\n",
      "|   37.5|         Raddish Red|2013-06-16|   40.0|   35.0|  8|  Kg|\n",
      "|   27.5|Raddish White(Local)|2013-06-16|   30.0|   25.0|  9|  Kg|\n",
      "|   17.0|        Brinjal Long|2013-06-16|   18.0|   16.0| 10|  Kg|\n",
      "|   21.0|       Brinjal Round|2013-06-16|   22.0|   20.0| 11|  Kg|\n",
      "|   22.5|       Cow pea(Long)|2013-06-16|   25.0|   20.0| 12|  Kg|\n",
      "|   57.5|          Green Peas|2013-06-16|   60.0|   55.0| 13|  Kg|\n",
      "|   27.5|  French Bean(Local)|2013-06-16|   30.0|   25.0| 14|  Kg|\n",
      "|   65.0|      Soyabean Green|2013-06-16|   70.0|   60.0| 15|  Kg|\n",
      "|   15.0|        Bitter Gourd|2013-06-16|   16.0|   14.0| 16|  Kg|\n",
      "|   17.5|        Bottle Gourd|2013-06-16|   20.0|   15.0| 17|  Kg|\n",
      "|   32.5|Pointed Gourd(Local)|2013-06-16|   35.0|   30.0| 18|  Kg|\n",
      "|   27.5|         Snake Gourd|2013-06-16|   30.0|   25.0| 19|  Kg|\n",
      "+-------+--------------------+----------+-------+-------+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 12:56:31 WARN TaskSetManager: Stage 118 contains a task of very large size (5701 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(spark.sparkContext.parallelize(kalimati_json))\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
